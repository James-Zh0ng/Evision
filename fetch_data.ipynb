{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from web3 import Web3, HTTPProvider\n",
    "\n",
    "# Get your node JSON-RPC URL\n",
    "json_rpc_url = \"https://mainnet.infura.io/v3/eb83c1f3e720455a8ebd9a940ba18ced\"\n",
    "web3 = Web3(HTTPProvider(json_rpc_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniswap-v3-poolcreated.csv created successfully!\n",
      "uniswap-v3-swap.csv created successfully!\n",
      "uniswap-v3-mint.csv created successfully!\n",
      "uniswap-v3-burn.csv created successfully!\n",
      "Operation completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder and file names with corresponding columns\n",
    "data_structure = {\n",
    "    'uniswap-v3-poolcreated.csv': [\n",
    "        \"block_number\", \"timestamp\", \"tx_hash\", \"log_index\", \"factory_contract_address\", \n",
    "        \"pool_contract_address\", \"fee\", \"token0_address\", \"token0_symbol\", \"token1_address\", \n",
    "        \"token1_symbol\"\n",
    "    ],\n",
    "    'uniswap-v3-swap.csv': [\n",
    "        \"block_number\", \"timestamp\", \"tx_hash\", \"log_index\", \"pool_contract_address\", \n",
    "        \"amount0\", \"amount1\", \"sqrt_price_x96\", \"liquidity\", \"tick\"\n",
    "    ],\n",
    "    'uniswap-v3-mint.csv': [\n",
    "        \"block_number\", \"timestamp\", \"tx_hash\", \"log_index\", \"pool_contract_address\", \n",
    "        \"tick_lower\", \"tick_upper\", \"amount\", \"amount0\", \"amount1\"\n",
    "    ],\n",
    "    'uniswap-v3-burn.csv': [\n",
    "        \"block_number\", \"timestamp\", \"tx_hash\", \"log_index\", \"pool_contract_address\", \n",
    "        \"tick_lower\", \"tick_upper\", \"amount\", \"amount0\", \"amount1\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "folder_name = 'data'\n",
    "\n",
    "# Create the 'data' folder if it does not exist\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Generate and save CSV files with specific columns\n",
    "for file_name, columns in data_structure.items():\n",
    "    # Define the file path\n",
    "    file_path = os.path.join(folder_name, file_name)\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(file_path):\n",
    "        # If not, create an empty DataFrame with specific columns\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        \n",
    "        # Save the DataFrame as a CSV file\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f'{file_name} created successfully!')\n",
    "    else:\n",
    "        print(f'{file_name} already exists.')\n",
    "\n",
    "print('Operation completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch all the on-chain transactions of Uniswap v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error HTTPSConnectionPool(host='mainnet.infura.io', port=443): Read timed out. (read timeout=10) when calling method:\n",
      "eth_chainId(())\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.100000 seconds, retry #0 / 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous scan done, starting fresh from block 12,369,621, total 50,000 blocks\n",
      "Scanning block range 12,369,621 - 12,419,621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fa05e0cc98410c98bde0232c107c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error HTTPSConnectionPool(host='mainnet.infura.io', port=443): Read timed out. (read timeout=10) when calling method:\n",
      "eth_chainId(())\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.100000 seconds, retry #0 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error HTTPSConnectionPool(host='mainnet.infura.io', port=443): Read timed out. (read timeout=10) when calling method:\n",
      "eth_chainId(())\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.100000 seconds, retry #0 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error HTTPSConnectionPool(host='mainnet.infura.io', port=443): Read timed out. (read timeout=10) when calling method:\n",
      "eth_chainId(())\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.100000 seconds, retry #0 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error HTTPSConnectionPool(host='mainnet.infura.io', port=443): Read timed out. (read timeout=10) when calling method:\n",
      "eth_chainId(())\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.100000 seconds, retry #0 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error HTTPSConnectionPool(host='mainnet.infura.io', port=443): Read timed out. (read timeout=10) when calling method:\n",
      "eth_chainId(())\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.100000 seconds, retry #0 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error HTTPSConnectionPool(host='mainnet.infura.io', port=443): Read timed out. (read timeout=10) when calling method:\n",
      "eth_chainId(())\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.100000 seconds, retry #0 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error HTTPSConnectionPool(host='mainnet.infura.io', port=443): Read timed out. (read timeout=10) when calling method:\n",
      "eth_chainId(())\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.100000 seconds, retry #0 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error HTTPSConnectionPool(host='mainnet.infura.io', port=443): Read timed out. (read timeout=10) when calling method:\n",
      "eth_chainId(())\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.100000 seconds, retry #0 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error HTTPSConnectionPool(host='mainnet.infura.io', port=443): Read timed out. (read timeout=10) when calling method:\n",
      "eth_chainId(())\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.100000 seconds, retry #0 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error {'code': -32000, 'message': 'execution reverted'} when calling method:\n",
      "eth_call(({'to': '0xC36442b4a4522E871399CD717aBDD847Ab11FE88', 'data': '0x313ce567'}, 'latest'))\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.100000 seconds, retry #0 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error {'code': -32000, 'message': 'execution reverted'} when calling method:\n",
      "eth_call(({'to': '0xC36442b4a4522E871399CD717aBDD847Ab11FE88', 'data': '0x313ce567'}, 'latest'))\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.110000 seconds, retry #1 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error {'code': -32000, 'message': 'execution reverted'} when calling method:\n",
      "eth_call(({'to': '0xC36442b4a4522E871399CD717aBDD847Ab11FE88', 'data': '0x313ce567'}, 'latest'))\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.121000 seconds, retry #2 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error {'code': -32000, 'message': 'execution reverted'} when calling method:\n",
      "eth_call(({'to': '0xC36442b4a4522E871399CD717aBDD847Ab11FE88', 'data': '0x313ce567'}, 'latest'))\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.133100 seconds, retry #3 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error {'code': -32000, 'message': 'execution reverted'} when calling method:\n",
      "eth_call(({'to': '0xC36442b4a4522E871399CD717aBDD847Ab11FE88', 'data': '0x313ce567'}, 'latest'))\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.146410 seconds, retry #4 / 6\n",
      "WARNING:eth_defi.provider.fallback:Encountered JSON-RPC retryable error {'code': -32000, 'message': 'execution reverted'} when calling method:\n",
      "eth_call(({'to': '0xC36442b4a4522E871399CD717aBDD847Ab11FE88', 'data': '0x313ce567'}, 'latest'))\n",
      "Switching providers mainnet.infura.io -> mainnet.infura.io\n",
      "Retrying in 0.161051 seconds, retry #5 / 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 855 PoolCreated events\n",
      "Wrote 230065 Swap events\n",
      "Wrote 14077 Mint events\n",
      "Wrote 4619 Burn events\n"
     ]
    }
   ],
   "source": [
    "from eth_defi.uniswap_v3.constants import UNISWAP_V3_FACTORY_CREATED_AT_BLOCK\n",
    "from eth_defi.uniswap_v3.events import fetch_events_to_csv\n",
    "from eth_defi.event_reader.json_state import JSONFileScanState\n",
    "\n",
    "start_block = UNISWAP_V3_FACTORY_CREATED_AT_BLOCK\n",
    "end_block = UNISWAP_V3_FACTORY_CREATED_AT_BLOCK + 50000\n",
    "\n",
    "# Stores the last block number of event data we store\n",
    "state = JSONFileScanState(\"/tmp/uniswap-v3-price-scan-12.json\")\n",
    " \n",
    "fetch_events_to_csv(json_rpc_url, state, output_folder=folder_name, end_block=end_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pseudo addresses for demonstration purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethereum addresses added to uniswap-v3-poolcreated.csv\n",
      "Ethereum addresses added to uniswap-v3-swap.csv\n",
      "Ethereum addresses added to uniswap-v3-mint.csv\n",
      "Ethereum addresses added to uniswap-v3-burn.csv\n",
      "Unique Ethereum addresses saved to unique_ethereum_addresses.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "def generate_ethereum_address():\n",
    "    return \"0x\" + uuid.uuid4().hex[:40]\n",
    "\n",
    "def add_ethereum_address(folder_name, file_names):\n",
    "    unique_addresses = set()\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_name, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['ethereum_address'] = [generate_ethereum_address() for _ in range(len(df))]\n",
    "            unique_addresses.update(df['ethereum_address'])\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f'Ethereum addresses added to {file_name}')\n",
    "        else:\n",
    "            print(f'{file_name} does not exist')\n",
    "\n",
    "    # Save the unique addresses to a CSV file\n",
    "    unique_addresses_df = pd.DataFrame(list(unique_addresses), columns=['ethereum_address'])\n",
    "    unique_addresses_df.to_csv(os.path.join(folder_name, 'uniswap-v3-user.csv'), index=False)\n",
    "    print('Unique Ethereum addresses saved to unique_ethereum_addresses.csv')\n",
    "\n",
    "file_names = list(data_structure.keys())\n",
    "\n",
    "add_ethereum_address(folder_name, file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch information of tokens on Uniswap v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique token addresses: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cd91af5544427db2d68d854c83a9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching token details:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens fetched: 14\n",
      "Data fetched and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Apply the patch to enable running asyncio code in Jupyter notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "ALCHEMY_API_KEY = 'Po5QDQwc-ejbU22kv3lM4bG8slMP52DW'\n",
    "BASE_URL = f'https://eth-mainnet.alchemyapi.io/v2/{ALCHEMY_API_KEY}'\n",
    "\n",
    "async def fetch(session, url, json):\n",
    "    async with session.post(url, json=json) as response:\n",
    "        if response.status != 200:\n",
    "            print(f\"Failed to fetch data: {await response.text()}\")\n",
    "            return None\n",
    "        return await response.json()\n",
    "\n",
    "async def get_token_details(session, token_address):\n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"id\": 1,\n",
    "        \"method\": \"alchemy_getTokenMetadata\",\n",
    "        \"params\": [token_address]\n",
    "    }\n",
    "    response = await fetch(session, BASE_URL, payload)\n",
    "    if response and 'result' in response:\n",
    "        token_data = response['result']\n",
    "        token_data['token_address'] = token_address  # Include token address in the result\n",
    "        return token_data\n",
    "    print(f\"Failed to get token details for {token_address}\")\n",
    "    return None\n",
    "\n",
    "async def fetch_all_token_details(token_addresses):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [\n",
    "            asyncio.ensure_future(get_token_details(session, token_address))\n",
    "            for token_address in tqdm(token_addresses, desc=\"Fetching token details\")\n",
    "        ]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "df = pd.read_csv('data/uniswap-v3-poolcreated.csv')\n",
    "\n",
    "# Get unique token addresses\n",
    "token_addresses = set(df['token0_address'].tolist() + df['token1_address'].tolist())\n",
    "\n",
    "print(f\"Total unique token addresses: {len(token_addresses)}\")\n",
    "tokens_data = await fetch_all_token_details(token_addresses)\n",
    "\n",
    "# Filter out None responses\n",
    "tokens_data = [token for token in tokens_data if token]\n",
    "print(f\"Number of tokens fetched: {len(tokens_data)}\")\n",
    "\n",
    "# Convert to DataFrame, ensuring 'address' is the first column\n",
    "column_order = ['token_address'] + [col for col in tokens_data[0] if col != 'token_address']\n",
    "tokens_df = pd.DataFrame(tokens_data)[column_order]\n",
    "\n",
    "tokens_df.to_csv(os.path.join(folder_name, 'uniswap-v3-token.csv'), index=False)\n",
    "\n",
    "print(\"Data fetched and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate liquidity pool entity from the four transaction logs table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liquidity pool information saved to data/liquidity_pool.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to the folders\n",
    "pool_created_file = os.path.join(folder_name, 'uniswap-v3-poolcreated.csv')\n",
    "swap_file = os.path.join(folder_name, 'uniswap-v3-swap.csv')\n",
    "output_file = os.path.join(folder_name, 'uniswap-v3-liquiditypool.csv')\n",
    "\n",
    "# Read the CSV files\n",
    "pool_created_df = pd.read_csv(pool_created_file)\n",
    "swap_df = pd.read_csv(swap_file)\n",
    "\n",
    "# Convert addresses to lower case for case-insensitive comparison\n",
    "pool_created_df['pool_contract_address'] = pool_created_df['pool_contract_address'].str.lower()\n",
    "swap_df['pool_contract_address'] = swap_df['pool_contract_address'].str.lower()\n",
    "\n",
    "# Convert the block number to integer for proper comparison\n",
    "swap_df['block_number'] = swap_df['block_number'].astype(int)\n",
    "\n",
    "# Drop duplicates from the pool_created_df based on the 'pool_contract_address' column\n",
    "pool_created_df = pool_created_df.drop_duplicates(subset='pool_contract_address')\n",
    "\n",
    "# Initialize the DataFrame to store the liquidity pool information\n",
    "liquidity_pool_data = {\n",
    "    'liquidity_pool_address': [],\n",
    "    'token0_address': [],\n",
    "    'token1_address': [],\n",
    "    'sqrtPriceX96': [],\n",
    "    'liquidity': []\n",
    "}\n",
    "\n",
    "# Iterate through the liquidity pools\n",
    "for index, row in pool_created_df.iterrows():\n",
    "    pool_address = row['pool_contract_address']\n",
    "    token0_address = row['token0_address']\n",
    "    token1_address = row['token1_address']\n",
    "    \n",
    "    # Find the latest swap interaction for the current liquidity pool\n",
    "    latest_swap = swap_df[swap_df['pool_contract_address'] == pool_address].sort_values(by=['block_number', 'log_index'], ascending=False).head(1)\n",
    "    \n",
    "    # Check if there is any swap interaction\n",
    "    if not latest_swap.empty:\n",
    "        sqrtPriceX96 = latest_swap.iloc[0]['sqrt_price_x96']\n",
    "        liquidity = latest_swap.iloc[0]['liquidity']\n",
    "    else:\n",
    "        sqrtPriceX96 = 0\n",
    "        liquidity = 0\n",
    "    \n",
    "    # Add the data to the dictionary\n",
    "    liquidity_pool_data['liquidity_pool_address'].append(pool_address)\n",
    "    liquidity_pool_data['token0_address'].append(token0_address)\n",
    "    liquidity_pool_data['token1_address'].append(token1_address)\n",
    "    liquidity_pool_data['sqrtPriceX96'].append(sqrtPriceX96)\n",
    "    liquidity_pool_data['liquidity'].append(liquidity)\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "liquidity_pool_df = pd.DataFrame(liquidity_pool_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "liquidity_pool_df.to_csv(output_file, index=False)\n",
    "\n",
    "print('Liquidity pool information saved to', output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify all the duplicate data that will be primary key in database schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def modify_duplicate(hash_value):\n",
    "    \"\"\"\n",
    "    Modify the given hash by replacing one of its characters.\n",
    "    \"\"\"\n",
    "    print(\"modifying the tx_hash: \" + hash_value)\n",
    "    # Convert the hash to a list\n",
    "    hash_list = list(hash_value)\n",
    "    \n",
    "    # Randomly select an index to modify (avoiding the '0x' prefix)\n",
    "    index = random.randint(2, len(hash_list) - 1)\n",
    "    \n",
    "    # Randomly select a hexadecimal character\n",
    "    hex_chars = '0123456789abcdef'\n",
    "    new_char = random.choice(hex_chars)\n",
    "    \n",
    "    # Ensure the new character is different\n",
    "    while new_char == hash_list[index]:\n",
    "        new_char = random.choice(hex_chars)\n",
    "    \n",
    "    # Replace the character at the selected index\n",
    "    hash_list[index] = new_char\n",
    "    \n",
    "    # Convert the list back to a string and return\n",
    "    return ''.join(hash_list)\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    \"\"\"\n",
    "    Keep finding and modifying duplicate hash values until there are no more duplicates.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Find duplicated tx_hash values\n",
    "        duplicated = df['tx_hash'].duplicated(keep=False)\n",
    "        \n",
    "        # If no duplicates, break out of the loop\n",
    "        if not duplicated.any():\n",
    "            break\n",
    "            \n",
    "        # Modify the duplicated tx_hash values\n",
    "        df.loc[duplicated, 'tx_hash'] = df.loc[duplicated, 'tx_hash'].apply(modify_duplicate)\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicated tx_hash values and modify them\n",
    "swap_df = pd.read_csv(swap_file)\n",
    "swap_df = remove_duplicates(swap_df)\n",
    "# Save the modified DataFrame to a new file\n",
    "swap_df.to_csv(swap_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicated tx_hash values and modify them\n",
    "create_df = pd.read_csv(pool_created_file)\n",
    "create_df = remove_duplicates(create_df)\n",
    "# Save the modified DataFrame to a new file\n",
    "create_df.to_csv(pool_created_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "mint_file = os.path.join(folder_name, 'uniswap-v3-mint.csv')\n",
    "mint_df = pd.read_csv(mint_file)\n",
    "mint_df = remove_duplicates(mint_df)\n",
    "# Save the modified DataFrame to a new file\n",
    "mint_df.to_csv(mint_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modifying the tx_hash: 0xfd9da3a9aa36fc28e5f5e70c8503dae321bf88e8143faea072b9cdf9a6783c67\n",
      "modifying the tx_hash: 0xfd9da3a9aa36fc28e5f5e70c8503dae321bf88e8143faea072b9cdf9a6783c67\n",
      "modifying the tx_hash: 0xf3719d3519a8a446cca050e1850f68623fc98ff085f240bc4a1f8a136e666be5\n",
      "modifying the tx_hash: 0xf3719d3519a8a446cca050e1850f68623fc98ff085f240bc4a1f8a136e666be5\n",
      "modifying the tx_hash: 0xbcf5afb528e6dadbb5e701278d2f7916472e0fa2b8227fa5ae0a2390b97ddaf3\n",
      "modifying the tx_hash: 0xbcf5afb528e6dadbb5e701278d2f7916472e0fa2b8227fa5ae0a2390b97ddaf3\n"
     ]
    }
   ],
   "source": [
    "burn_file = os.path.join(folder_name, 'uniswap-v3-burn.csv')\n",
    "burn_df = pd.read_csv(burn_file)\n",
    "burn_df = remove_duplicates(burn_df)\n",
    "burn_df.to_csv(burn_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
